{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"###### !pip uninstall tensorflow tensorflow_hub tensorflowjs\n# !pip install tensorflow==2.0.0a0 tensorflow_hub==0.5.0 tensorflowjs==1.2.6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T04:17:07.542695Z","iopub.execute_input":"2022-04-26T04:17:07.543522Z","iopub.status.idle":"2022-04-26T04:17:07.562178Z","shell.execute_reply.started":"2022-04-26T04:17:07.543423Z","shell.execute_reply":"2022-04-26T04:17:07.56147Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.compat.v1 as tf\n#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\ntf.disable_eager_execution()\nimport tensorflow_hub as hub\nfrom keras import backend as K\nfrom keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom sklearn.metrics import f1_score\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n# from sklearn import cross_validation\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:48:25.860340Z","iopub.execute_input":"2022-05-01T17:48:25.860840Z","iopub.status.idle":"2022-05-01T17:48:25.868063Z","shell.execute_reply.started":"2022-05-01T17:48:25.860803Z","shell.execute_reply":"2022-05-01T17:48:25.866908Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"1\"].values.tolist(),\n                                                           s[\"2\"].values.tolist())]\n        self.grouped = self.data.groupby(\"0\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:48:25.873947Z","iopub.execute_input":"2022-05-01T17:48:25.874512Z","iopub.status.idle":"2022-05-01T17:48:25.883928Z","shell.execute_reply.started":"2022-05-01T17:48:25.874483Z","shell.execute_reply":"2022-05-01T17:48:25.883267Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"max_len = 0\nn_tags = 0\ndef preprocess(path):\n    global max_len, n_tags\n    data = pd.read_csv(path, encoding=\"latin1\")\n    data = data.fillna(method=\"ffill\")\n    print(data.tail(10))\n\n    words = list(set(data[\"1\"].values))\n    words.append(\"ENDPAD\")\n    n_words = len(words)\n    tags = list(set(data[\"2\"].values))\n    n_tags = len(tags)\n    print(n_tags)\n\n    getter = SentenceGetter(data)\n    sent = getter.get_next()\n    print(sent)\n    sentences = getter.sentences\n    max_len = 50\n    tag2idx = {t: i for i, t in enumerate(tags)}\n#     print(\"this one here\", tag2idx[\"o\"])\n    X = [[w[0] for w in s] for s in sentences]\n    new_X = []\n    for seq in X:\n        new_seq = []\n        for i in range(max_len):\n            try:\n                new_seq.append(seq[i])\n            except:\n                new_seq.append(\"__PAD__\")\n        new_X.append(new_seq)\n    X = new_X\n\n    y = [[tag2idx[w[1]] for w in s] for s in sentences]\n    from keras.preprocessing.sequence import pad_sequences\n    y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n    return X, y, tag2idx","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:48:25.891977Z","iopub.execute_input":"2022-05-01T17:48:25.892415Z","iopub.status.idle":"2022-05-01T17:48:25.904313Z","shell.execute_reply.started":"2022-05-01T17:48:25.892370Z","shell.execute_reply":"2022-05-01T17:48:25.903320Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"path_to_file = \"../input/d/datasets/spashal/nlp-project/big_train.csv\"\nX_tr, y_tr, tag2idx = preprocess(path_to_file)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:48:25.945462Z","iopub.execute_input":"2022-05-01T17:48:25.945995Z","iopub.status.idle":"2022-05-01T17:48:26.169443Z","shell.execute_reply.started":"2022-05-01T17:48:25.945954Z","shell.execute_reply":"2022-05-01T17:48:26.168706Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"\nbatch_size = 32\nsess = tf.compat.v1.Session()\nK.set_session(sess)\n\nelmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\nsess.run(tf.global_variables_initializer())\nsess.run(tf.tables_initializer())\n\ndef ElmoEmbedding(x):\n    return elmo_model(inputs={\n                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n                            \"sequence_len\": tf.constant(batch_size*[max_len])\n                      },\n                      signature=\"tokens\",\n                      as_dict=True)[\"elmo\"]\n\ninput_text = Input(shape=(max_len,), dtype=tf.string)\nembedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\nx = Bidirectional(LSTM(units=512, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(embedding)\nx_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(x)\nx = add([x, x_rnn])  # residual connection to the first biLSTM\nout = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n\n'''\ninput = Input(shape=(140,))\nmodel = Embedding(input_dim=n_words, output_dim=140, input_length=140)(input)\nmodel = Dropout(0.1)(model)\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\nout = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n'''\n\ndef f1_scores(y_true, y_pred):\n    print(len(y_true), len(y_pred))\n#     y_t = tf.constant(y_true)\n#     print(type(y_true), type(y_pred))\n#     y_t = []\n#     print(y_true, y_pred)\n#     for val in y_true:\n#         y_t.append(int(val))\n#     y_p = []\n#     for val in y_pred:\n#         y_p.append(int(val))\n# #     y_p = tf.constant(y_pred)\n#     y_p = y_pred.eval(session=sess)\n    return f1_score(np.array(y_true, dtype=int), np.array(y_pred, dtype=int), average='weighted')\n\nmodel = Model(input_text, out)\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n# print(len(X_tr), len(y_tr))\nX_tr = X_tr[1:33]\ny_tr = y_tr[1:33]\nX_val, y_val, templar = preprocess('../input/d/datasets/spashal/nlp-project/valid.csv')\nX_val = X_val[:32]\n# X_val = X_val[:len(X_val) - len(X_val)%32]\n# x_val_temp = X_val.copy()\n# y_val = y_val[:len(y_val) - len(y_val)%32]\ny_val = y_val[:32]\ny_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\ny_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\nprint(np.array(X_val).shape)\nhistory_initial = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val), batch_size=batch_size, epochs=1, verbose=1)\ny_pred = model.predict(np.array(X_val))\ny_pred_n = []\nfor a in y_pred:\n    y_pred_n.append([])\n    for b in a:\n#         print(\"haha\", np.argmax(c)+1)\n        y_pred_n[-1].append(np.argmax(b)+1)\n#         for c in b:\n#             y_pred_n[-1][-1].append()\nprint(y_pred)\nprint(\"F1 score:\", f1_scores(y_val.flatten(), np.array(y_pred_n).flatten()))\n# bs = cross_validation.Bootstrap(len(X_tr), random_state=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\n\n# store the already trained model\nkeras.models.save_model(model, './initial_trained_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:49:15.454271Z","iopub.status.idle":"2022-05-01T17:49:15.454994Z","shell.execute_reply.started":"2022-05-01T17:49:15.454734Z","shell.execute_reply":"2022-05-01T17:49:15.454761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the pretrained model\nfrom tensorflow import keras\nsaved_model = keras.models.load_model('./initial_trained_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:49:15.456300Z","iopub.status.idle":"2022-05-01T17:49:15.457041Z","shell.execute_reply.started":"2022-05-01T17:49:15.456797Z","shell.execute_reply":"2022-05-01T17:49:15.456823Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(32):\n    if len(X_val[i]) != 50:\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:49:15.458544Z","iopub.status.idle":"2022-05-01T17:49:15.459314Z","shell.execute_reply.started":"2022-05-01T17:49:15.459034Z","shell.execute_reply":"2022-05-01T17:49:15.459061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ** Remember to store all the intermediate models to avoid any extra training **\n# tf.enable_eager_execution()\n# remove the final dense layer and replace with a new one\nX_tr, y_tr, tag2idx = preprocess(\"../input/d/datasets/spashal/nlp-project/10k_dataset_snorkel.csv\")\nX_tr, X_val = X_tr[:352], X_tr[352:384]\ny_tr, y_val = y_tr[:352], y_tr[352:384]\ny_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\ny_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\nmodel = saved_model\nmodel = Model(inputs=model.input, outputs=model.layers[-2].output)\n__num_of_classes = 22\n\nnew_model = keras.Sequential()\nnew_model.add(model)\n# new_model.add(keras.layers.Dense(100, activation='softmax'))\nnew_model.add(keras.layers.Dense(__num_of_classes, activation='softmax'))\n# model = new_model\n\n# freeze the embedding and lstm layers\nnew_model.layers[0].trainable = False\n# new_model.layers[1].trainable = False\n\n# train the new dense layer\ntf.compat.v1.experimental.output_all_intermediates(True)\nnew_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nnew_model.summary()\nhistory_intermediate = new_model.fit(x=np.array(X_tr),\n              y=y_tr,\n              batch_size=batch_size,\n              validation_data=(np.array(X_val),y_val),\n              epochs=20, verbose=1)\nkeras.models.save_model(new_model, './intermediate_trained_model.h5')\n# unfreeze and finetune the other layers \nnew_model.layers[0].trainable = True\nnew_model.layers[1].trainable = True\n\nnew_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nhistory_final = new_model.fit(x=np.array(X_tr),\n              y=y_tr,\n              batch_size=batch_size,\n              validation_data=(np.array(X_val),y_val),\n              epochs=100, verbose=1)\nkeras.models.save_model(new_model, './finetuned_trained_model.h5')\n\n","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:49:15.460502Z","iopub.status.idle":"2022-05-01T17:49:15.461262Z","shell.execute_reply.started":"2022-05-01T17:49:15.461012Z","shell.execute_reply":"2022-05-01T17:49:15.461039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nabc = dict()\nabc[\"initial\"] = history_initial.history\nabc[\"intermediate\"] = history_intermediate.history\nabc[\"final\"] = history_final.history\n\nmetrics = pd.DataFrame(abc)\nmetrics.to_csv('./metrics.csv')\n# json_object = json.dumps(dictionary, indent = 4) \n# with open(\"./metrics\", \"w\") as fp:\n#     json.dump(abc, fp, indent=4) ","metadata":{"execution":{"iopub.status.busy":"2022-05-01T17:49:15.462656Z","iopub.status.idle":"2022-05-01T17:49:15.463064Z","shell.execute_reply.started":"2022-05-01T17:49:15.462843Z","shell.execute_reply":"2022-05-01T17:49:15.462866Z"},"trusted":true},"execution_count":null,"outputs":[]}]}