{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# !pip uninstall tensorflow tensorflow_hub tensorflowjs\n# !pip install tensorflow==2.0.0a0 tensorflow_hub==0.5.0 tensorflowjs==1.2.6","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T04:17:07.542695Z","iopub.execute_input":"2022-04-26T04:17:07.543522Z","iopub.status.idle":"2022-04-26T04:17:07.562178Z","shell.execute_reply.started":"2022-04-26T04:17:07.543423Z","shell.execute_reply":"2022-04-26T04:17:07.56147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\nfrom sklearn.model_selection import train_test_split\nimport tensorflow.compat.v1 as tf\n#To make tf 2.0 compatible with tf1.0 code, we disable the tf2.0 functionalities\ntf.disable_eager_execution()\nimport tensorflow_hub as hub\nfrom keras import backend as K\nfrom keras.models import Model, Input\nfrom keras.layers.merge import add\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n# from sklearn import cross_validation\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:17:07.573775Z","iopub.execute_input":"2022-04-26T04:17:07.574011Z","iopub.status.idle":"2022-04-26T04:17:13.193695Z","shell.execute_reply.started":"2022-04-26T04:17:07.573962Z","shell.execute_reply":"2022-04-26T04:17:13.192953Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"1\"].values.tolist(),\n                                                           s[\"2\"].values.tolist())]\n        self.grouped = self.data.groupby(\"0\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None\n\n\ndata = pd.read_csv(\"../input/d/datasets/spashal/nlp-project/10k_dataset.csv\", encoding=\"latin1\")\ndata = data.fillna(method=\"ffill\")\nprint(data.tail(10))\n\nwords = list(set(data[\"1\"].values))\nwords.append(\"ENDPAD\")\nn_words = len(words)\ntags = list(set(data[\"2\"].values))\nn_tags = len(tags)\nprint(n_tags)\n\ngetter = SentenceGetter(data)\nsent = getter.get_next()\nprint(sent)\nsentences = getter.sentences\nmax_len = 50\ntag2idx = {t: i for i, t in enumerate(tags)}\nX = [[w[0] for w in s] for s in sentences]\nnew_X = []\nfor seq in X:\n    new_seq = []\n    for i in range(max_len):\n        try:\n            new_seq.append(seq[i])\n        except:\n            new_seq.append(\"__PAD__\")\n    new_X.append(new_seq)\nX = new_X\nprint(X[1])\n\ny = [[tag2idx[w[1]] for w in s] for s in sentences]\nfrom keras.preprocessing.sequence import pad_sequences\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\nprint(y[1])\n\nX_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, random_state=2018)\nbatch_size = 32\nsess = tf.compat.v1.Session()\nK.set_session(sess)\n\nelmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\nsess.run(tf.global_variables_initializer())\nsess.run(tf.tables_initializer())\n\ndef ElmoEmbedding(x):\n    return elmo_model(inputs={\n                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n                            \"sequence_len\": tf.constant(batch_size*[max_len])\n                      },\n                      signature=\"tokens\",\n                      as_dict=True)[\"elmo\"]\n\ninput_text = Input(shape=(max_len,), dtype=tf.string)\nembedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\nx = Bidirectional(LSTM(units=512, return_sequences=True,\n                       recurrent_dropout=0.2, dropout=0.2))(embedding)\nx_rnn = Bidirectional(LSTM(units=512, return_sequences=True,\n                           recurrent_dropout=0.2, dropout=0.2))(x)\nx = add([x, x_rnn])  # residual connection to the first biLSTM\nout = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n\n'''\ninput = Input(shape=(140,))\nmodel = Embedding(input_dim=n_words, output_dim=140, input_length=140)(input)\nmodel = Dropout(0.1)(model)\nmodel = Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.1))(model)\nout = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(model)  # softmax output layer\n'''\n\nmodel = Model(input_text, out)\nmodel.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n# print(len(X_tr), len(y_tr))\nX_tr, X_val = X_tr[:320], X_tr[320:352]\ny_tr, y_val = y_tr[:320], y_tr[320:352]\ny_tr = y_tr.reshape(y_tr.shape[0], y_tr.shape[1], 1)\ny_val = y_val.reshape(y_val.shape[0], y_val.shape[1], 1)\n\nhistory = model.fit(np.array(X_tr), y_tr, validation_data=(np.array(X_val), y_val), batch_size=batch_size, epochs=3, verbose=1)\n\n# bs = cross_validation.Bootstrap(len(X_tr), random_state=0)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:17:13.195308Z","iopub.execute_input":"2022-04-26T04:17:13.195745Z","iopub.status.idle":"2022-04-26T04:17:59.738585Z","shell.execute_reply.started":"2022-04-26T04:17:13.195713Z","shell.execute_reply":"2022-04-26T04:17:59.737802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import keras\n\n# store the already trained model\nkeras.models.save_model(model, './initial_trained_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:17:59.740245Z","iopub.execute_input":"2022-04-26T04:17:59.740788Z","iopub.status.idle":"2022-04-26T04:18:01.362756Z","shell.execute_reply.started":"2022-04-26T04:17:59.740747Z","shell.execute_reply":"2022-04-26T04:18:01.362033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# loading the pretrained model\nfrom tensorflow import keras\nsaved_model = keras.models.load_model('./initial_trained_model.h5')","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:18:01.364559Z","iopub.execute_input":"2022-04-26T04:18:01.36481Z","iopub.status.idle":"2022-04-26T04:18:13.584119Z","shell.execute_reply.started":"2022-04-26T04:18:01.364777Z","shell.execute_reply":"2022-04-26T04:18:13.583251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(32):\n    if len(X_val[i]) != 50:\n        print(i)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:18:13.585497Z","iopub.execute_input":"2022-04-26T04:18:13.585778Z","iopub.status.idle":"2022-04-26T04:18:13.590609Z","shell.execute_reply.started":"2022-04-26T04:18:13.585743Z","shell.execute_reply":"2022-04-26T04:18:13.589944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ** Remember to store all the intermediate models to avoid any extra training **\n# tf.enable_eager_execution()\n# remove the final dense layer and replace with a new one\n\nmodel = saved_model\nmodel = Model(inputs=model.input, outputs=model.layers[-2].output)\n__num_of_classes = 22\n\nnew_model = keras.Sequential()\nnew_model.add(model)\n# new_model.add(keras.layers.Dense(100, activation='softmax'))\nnew_model.add(keras.layers.Dense(__num_of_classes, activation='softmax'))\n# model = new_model\n\n# freeze the embedding and lstm layers\nnew_model.layers[0].trainable = False\n# new_model.layers[1].trainable = False\n\n# train the new dense layer\ntf.compat.v1.experimental.output_all_intermediates(True)\nnew_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nnew_model.summary()\nnew_model.fit(x=np.array(X_tr),\n              y=y_tr,\n              batch_size=batch_size,\n              validation_data=(np.array(X_val),y_val),\n              epochs=20, verbose=1)\n\n# unfreeze and finetune the other layers \nnew_model.layers[0].trainable = True\nnew_model.layers[1].trainable = True\n\nnew_model.compile(optimizer=\"sgd\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nnew_model.fit(x=np.array(X_tr),\n              y=y_tr,\n              batch_size=batch_size,\n              validation_data=(np.array(X_val),y_val),\n              epochs=100, verbose=1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T04:26:07.233134Z","iopub.execute_input":"2022-04-26T04:26:07.233394Z","iopub.status.idle":"2022-04-26T04:37:13.307754Z","shell.execute_reply.started":"2022-04-26T04:26:07.233366Z","shell.execute_reply":"2022-04-26T04:37:13.306981Z"},"trusted":true},"execution_count":null,"outputs":[]}]}